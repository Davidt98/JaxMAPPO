# For comparison with r2i
"LR": 3.5e-4 # used 3.5 in all runs! except for the run where 2.5 is written see slides
# "LR": 3.5e-4 # siehe presentation, where LR: 3.5 wasnt specified 2.5 waas prob used
"NUM_ENVS": 48 # final presentation param
"NUM_STEPS": 256 # final presentation param
# "LR": 3.5e-4
# "NUM_ENVS": 128 # final presentation for code not using r2i impl
# "NUM_STEPS": 512 # final presentation for code not using r2i impl
# "TOTAL_TIMESTEPS": 7.5e+6
# "TOTAL_TIMESTEPS": 1.0e+7 # 6.5e6 for final presentation run.. nochmal nachrechnen if doubts..
"TOTAL_TIMESTEPS": 7.5e+6

# For comparison with IPPO vanilla
# "LR": 3.5e-4
# "NUM_ENVS": 128
# "NUM_STEPS": 512
# "TOTAL_TIMESTEPS": 7.0e+7
"UPDATE_EPOCHS": 4
"NUM_MINIBATCHES": 4
"GAMMA": 0.99 
"GAE_LAMBDA": 0.95 
"CLIP_EPS": 0.2 
"SCALE_CLIP_EPS": False
"ENT_COEF": 0.01
"VF_COEF": 0.5 
"MAX_GRAD_NORM": 0.5
"ACTIVATION": "tanh"
"ENV_NAME": "overcooked"
# "ENV_KWARGS": 
#   "layout" : "cramped_room"
"LAYOUTS": ["cramped_room", "asymm_advantages", "coord_ring", "forced_coord", "counter_circuit"]
# "LAYOUTS": ["coord_ring", "forced_coord", "counter_circuit"]
# "LAYOUTS": ["coord_ring"]
"ANNEAL_LR": True 
"SEED": 0
"NUM_SEEDS": 3

# batch: subset of the training
# minibatch: subset of batch

# WandB Params
"WANDB_MODE": "online"
"ENTITY": "thichqwerty"
"PROJECT": "r2i_mappo_overcooked"
# TODO: Remove comments
# r2i
