"LR": 2.5e-4 # learning rate, see calculation of gradient of the loss function

"NUM_ENVS": 200 # number of parallel env run (simultaneously) 16
"NUM_STEPS": 400 # number of steps before model is updated
"TOTAL_TIMESTEPS": 1.0e+7 # 5.0e+6 # total number of steps (actions) taken during training
"UPDATE_EPOCHS": 4 # model is updated x times per batch
"NUM_MINIBATCHES": 4
"GAMMA": 0.99 # discount factor (determines importance for future rewards)
"GAE_LAMBDA": 0.95 # generalized advantage estimation (balances bias-variance trade-off)
"CLIP_EPS": 0.2 # prevents large policy updates (by clipping ratio between old and new policy)
"SCALE_CLIP_EPS": False
"ENT_COEF": 0.01 # encourages exploration
"VF_COEF": 0.5 # value funciton coefficient (balances between policy improvement and value estimation)
"MAX_GRAD_NORM": 0.5 # specifies maximum gradient norm allowed during training
"ACTIVATION": "tanh"
"ENV_NAME": "overcooked"
# "ENV_KWARGS": 
#   "layout" : "cramped_room"
# "LAYOUTS": ["cramped_room", "asymm_advantages", "coord_ring", "forced_coord", "counter_circuit"]
"LAYOUTS": ["asymm_advantages"]
"ANNEAL_LR": True # determines whether or not to anneal (gradually decrease) the learning rate during training
"SEED": 0
"NUM_SEEDS": 3

# batch: subset of the training
# minibatch: subset of batch

# WandB Params
"WANDB_MODE": "online"
"ENTITY": "thichqwerty"
"PROJECT": "r2i_mappo_overcooked"